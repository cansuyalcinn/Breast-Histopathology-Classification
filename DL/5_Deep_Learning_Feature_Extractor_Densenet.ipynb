{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJIvCP90W5hF"
      },
      "source": [
        "# Extract Features using Transfer Learning Densenet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains the code of the feature extraction using DenseNet121 from Tensorflow Keras API. As an output, we obtained feature vector with the dimesnions of N x 1024 where N is the number of samples."
      ],
      "metadata": {
        "id": "_UhOeGjquei_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "jIvsBAHAuusT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0EBWu70OUUK",
        "outputId": "d62faf0d-7877-4548-8a5a-54fc51fea143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsYt3eqeWly8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "import os\n",
        "import cv2 as cv\n",
        "from PIL import Image\n",
        "import os\n",
        "import pickle\n",
        "import cv2\n",
        "from tensorflow.keras.utils import normalize\n",
        "from PIL import Image\n",
        "import glob\n",
        "import numpy as np\n",
        "plt.style.use('classic')\n",
        "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) \n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to Obtain Feature vector of a given path of images\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iEaujFkShvsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image paths\n",
        "paths_ml = pickle.load(open('/content/drive/MyDrive/Ars_machinae_autodiscentis/Inceptum/paths.p','rb'))"
      ],
      "metadata": {
        "id": "yA0Cpjb2iGm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_model(image):\n",
        "\n",
        "  image = tf.keras.applications.densenet.preprocess_input(image)\n",
        "  SIZE=460\n",
        "  densenet = tf.keras.applications.DenseNet121(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))\n",
        "\n",
        "  for layer in densenet.layers:\n",
        "    layer.trainable = False\n",
        "  \n",
        "  densenet.summary()\n",
        "  model = Sequential()\n",
        "  model.add(densenet)\n",
        "  model.add(GlobalAveragePooling2D())\n",
        "\n",
        "  feature_extractor = model.predict(image)\n",
        "  features = feature_extractor.reshape(feature_extractor.shape[0], -1)\n",
        "  X_for_RF_d = features \n",
        "  return X_for_RF_d"
      ],
      "metadata": {
        "id": "rwMqi_u8jMxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_images(paths_ml):\n",
        "  images = []\n",
        "  for path in tqdm(paths_ml):\n",
        "    im = cv.imread(path)\n",
        "    im = cv.resize(im, (460,460))\n",
        "    images.append(im)\n",
        "  return images"
      ],
      "metadata": {
        "id": "8jcKwVvQhuFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_fv(paths,feature_name,fold, train_test, magnification):\n",
        "  mag_dictionary = {0:'40',1:'100',2:'200',3:'400'}\n",
        "  train_dic = {0:'train',1:'test'}\n",
        "  images = np.array(read_images(paths[fold,train_test,magnification]))\n",
        "  fv = dense_model(images)\n",
        "  os.chdir('/content/drive/MyDrive/Ars_machinae_autodiscentis/Inceptum/Machine_Learning/pickle_files')\n",
        "  pickle.dump(fv ,open(f\"{feature_name}_f{fold+1}_{train_dic[train_test]}_{mag_dictionary[magnification]}x_fv.p\",\"wb\"))"
      ],
      "metadata": {
        "id": "JH2u5OI_9voS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold in range(0,5):\n",
        "   for tt in range(2):\n",
        "     for mag in range(4):\n",
        "       get_file_fv(paths=paths_ml,feature_name = 'dense',fold=fold,train_test=tt,magnification=mag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evk9_Pyi9sw0",
        "outputId": "94ab490b-fa99-4db7-e387-16c64fc529e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:20<00:00, 61.07it/s]\n",
            "100%|██████████| 1321/1321 [07:54<00:00,  2.78it/s]\n",
            "100%|██████████| 1269/1269 [07:13<00:00,  2.93it/s]\n",
            "100%|██████████| 1165/1165 [06:38<00:00,  2.93it/s]\n",
            "100%|██████████| 745/745 [00:13<00:00, 54.63it/s]\n",
            "100%|██████████| 760/760 [04:53<00:00,  2.59it/s]\n",
            "100%|██████████| 744/744 [04:13<00:00,  2.93it/s]\n",
            "100%|██████████| 655/655 [03:37<00:00,  3.01it/s]\n",
            "100%|██████████| 1363/1363 [05:33<00:00,  4.09it/s]\n",
            "100%|██████████| 1461/1461 [05:38<00:00,  4.32it/s]\n",
            "100%|██████████| 1416/1416 [05:31<00:00,  4.27it/s]\n",
            "100%|██████████| 1266/1266 [05:13<00:00,  4.03it/s]\n",
            "100%|██████████| 632/632 [02:44<00:00,  3.83it/s]\n",
            "100%|██████████| 620/620 [02:35<00:00,  3.99it/s]\n",
            "100%|██████████| 597/597 [02:24<00:00,  4.14it/s]\n",
            "100%|██████████| 554/554 [02:16<00:00,  4.06it/s]\n",
            "100%|██████████| 1338/1338 [06:01<00:00,  3.70it/s]\n",
            "100%|██████████| 1412/1412 [06:02<00:00,  3.89it/s]\n",
            "100%|██████████| 1354/1354 [05:40<00:00,  3.98it/s]\n",
            "100%|██████████| 1228/1228 [04:36<00:00,  4.44it/s]\n",
            "100%|██████████| 657/657 [02:45<00:00,  3.97it/s]\n",
            "100%|██████████| 669/669 [02:45<00:00,  4.04it/s]\n",
            "100%|██████████| 659/659 [02:35<00:00,  4.23it/s]\n",
            "100%|██████████| 592/592 [02:20<00:00,  4.22it/s]\n",
            "100%|██████████| 1295/1295 [05:08<00:00,  4.20it/s]\n",
            "100%|██████████| 1383/1383 [06:11<00:00,  3.73it/s]\n",
            "100%|██████████| 1334/1334 [05:12<00:00,  4.27it/s]\n",
            "100%|██████████| 1199/1199 [04:02<00:00,  4.95it/s]\n",
            "100%|██████████| 700/700 [02:31<00:00,  4.63it/s]\n",
            "100%|██████████| 698/698 [02:33<00:00,  4.56it/s]\n",
            "100%|██████████| 679/679 [02:20<00:00,  4.84it/s]\n",
            "100%|██████████| 621/621 [02:07<00:00,  4.87it/s]\n",
            "100%|██████████| 1220/1220 [04:17<00:00,  4.74it/s]\n",
            "100%|██████████| 1260/1260 [04:12<00:00,  4.99it/s]\n",
            "100%|██████████| 1243/1243 [04:03<00:00,  5.10it/s]\n",
            "100%|██████████| 1103/1103 [03:41<00:00,  4.98it/s]\n",
            "100%|██████████| 775/775 [02:54<00:00,  4.44it/s]\n",
            "100%|██████████| 821/821 [03:00<00:00,  4.56it/s]\n",
            "100%|██████████| 770/770 [02:41<00:00,  4.78it/s]\n",
            "100%|██████████| 717/717 [02:29<00:00,  4.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Ars_machinae_autodiscentis/Inceptum/Machine_Learning/pickle_files')\n",
        "pickle.dump(pd.DataFrame(vector_f1_40_train),open(f\"dense_f1_train_40x_fv.p\",\"wb\"))"
      ],
      "metadata": {
        "id": "_wC_O2S2vnKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Ars_machinae_autodiscentis/Inceptum/Machine_Learning/pickle_files')\n",
        "pickle.dump(pd.DataFrame(vector_f1_40_test),open(f\"dense_f1_test_40x_fv.p\",\"wb\"))"
      ],
      "metadata": {
        "id": "qnErTp4uv3Bb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jIvsBAHAuusT",
        "iEaujFkShvsA"
      ],
      "name": "5.Deep_Learning_Feature_Extractor_Densenet.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}