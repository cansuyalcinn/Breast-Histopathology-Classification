{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.Machine_Learning_Multiclass",
      "provenance": [],
      "collapsed_sections": [
        "2ffyvGiA4hSn",
        "U6LKXU4a4nTs",
        "5WuCrHRa5VDi",
        "Va9YcbgI5R1R",
        "B21Tpx4r5fFq",
        "MXtv7ef-z_It"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCcRZTB84XU9",
        "outputId": "183283b0-1d13-49e5-8bb4-0be85dcce162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "2ffyvGiA4hSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import skimage\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import copy\n",
        "\n",
        "#Model creation\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "import scipy\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.metrics import matthews_corrcoef, make_scorer\n",
        "\n",
        "#Oversamplig for unbalance\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC\n",
        "from imblearn.pipeline import Pipeline as Pipelineim\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ],
      "metadata": {
        "id": "xVAEyq794Y4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "U6LKXU4a4nTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Global dictionaries\n",
        "mag_dict = {0:'40',1:'100',2:'200',3:'400'}\n",
        "tt_dict = {0:'train',1:'test'}"
      ],
      "metadata": {
        "id": "fWWDJoyg4kkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pickle-based functions"
      ],
      "metadata": {
        "id": "5WuCrHRa5VDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_from_pickle(category, fold, mag):\n",
        "  \"\"\"\n",
        "  Loads feature matrices, enpoints or patient's ID for each folder and magnification.\n",
        "  It returns the train and test arrays of the selected category\n",
        "  \n",
        "  :param category: Category of the data (feature matrices (X), Enpoints (y) or patient's ID)\n",
        "  :param f: Fold\n",
        "  :param mag: Magnification\n",
        "  :return: two np.arrays, train and test\n",
        "  \"\"\"\n",
        "  path_train = f'/content/drive/MyDrive/Ars_machinae_autodiscentis/Inceptum/fold{fold+1}/train/{category}_f{fold+1}_train_{mag_dict[mag]}x_fv.p'\n",
        "  path_test = f'/content/drive/MyDrive/Ars_machinae_autodiscentis/Inceptum/fold{fold+1}/test/{category}_f{fold+1}_test_{mag_dict[mag]}x_fv.p'\n",
        "  with open(path_train,'rb') as handle:\n",
        "    train_array = pickle.load(handle)\n",
        "  with open(path_test, 'rb') as handle:\n",
        "    test_array = pickle.load(handle)\n",
        "  \n",
        "  return train_array, test_array\n",
        "\n",
        "def read_files_pickle(extractor, fold, mag): #Name given to mimic csv files function\n",
        "  \"\"\"\n",
        "  Given the feature extractor name, folder nad magnifications, it returns the train-test split for the three categories (X, y, Patient ID)\n",
        "\n",
        "  :param extractor: feature extractor name. e.g. extractor='GLCM'\n",
        "  :param fold: fold -1\n",
        "  :param mag: magnification following the mag_dicitonary indexing\n",
        "  :return: train-test split\n",
        "  \"\"\"\n",
        "  X_train, X_test = train_test_from_pickle(category=extractor, fold=fold, mag=mag)\n",
        "  y_train, y_test = train_test_from_pickle(category='endpoints', fold=fold, mag=mag)\n",
        "  ID_train, ID_test = train_test_from_pickle(category='ID', fold=fold, mag=mag)\n",
        "\n",
        "  return X_train, X_test, y_train, y_test, ID_train, ID_test\n",
        "\n",
        "def remove_correlated(X_train, X_test, max_corr):\n",
        "  \"\"\"\n",
        "  Removes high correlated features. Input can be np. array of pandas df, but output will be pandas df.\n",
        "  :param X_train: training feature matrix\n",
        "  :param X_test: testing feature matrix\n",
        "  :param corr: Maximum correlation accepted ebtween features.\n",
        "  :return: Training and test un correlated feature matrices as dataframe\n",
        "  \"\"\"\n",
        "  X_train = pd.DataFrame(X_train)\n",
        "  X_test = pd.DataFrame(X_test)\n",
        "  cor_matrix = X_train.corr().abs()\n",
        "  upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n",
        "  to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > max_corr)]\n",
        "  X_train_nocorr = X_train.drop(X_train[to_drop], axis=1)\n",
        "  X_test_nocorr = X_test.drop(X_train[to_drop], axis=1)\n",
        "  \n",
        "  return X_train_nocorr, X_test_nocorr\n",
        "\n",
        "def machine_learning_pipeline(X_train, X_test, y_train, y_test, ID_test, pipe, max_corr=1):\n",
        "  \"\"\"\n",
        "  Main machine_learning pipeline.It receives the data (feature matrices, label and patients' ID) and produces the csv with the comparison between real and predicted labes. The model is also returned.\n",
        "  \n",
        "  :param X_train, X_test...etc: feature matrices, labels and patients' ID. X matrices are dataframes, the rest are np.arrays\n",
        "  :param pipe: Pipeline defined using sklearn\n",
        "  :param max_corr: Maximum correlation accepted among features\n",
        "  :return: dataframe with the ID, prediction and real label, and model used\n",
        "  \"\"\"\n",
        "  print(f'Input features: {X_train.shape[1]}\\n')\n",
        "  X_train_nocorr, X_test_nocorr = remove_correlated(X_train, X_test, max_corr=max_corr) #Remove correlated features from X_train and X_test\n",
        "  print(f'No-correlated features: {X_train_nocorr.shape[1]}\\n')\n",
        "  model = pipe.fit(X_train_nocorr, y_train.ravel()) #Model fit\n",
        "  y_pred = model.predict(X_test_nocorr) #Prediction\n",
        "\n",
        "  y_pred = pd.DataFrame(y_pred) # Dataframe of predictions\n",
        "  y_pred.columns = ['y_pred']\n",
        "  y_test = pd.DataFrame(y_test) # Dataframe of test labels\n",
        "  y_test.columns = ['y_test']\n",
        "  df_comparison = pd.DataFrame(ID_test).copy() # Copy the main dataframe with Patient ID's for test set.\n",
        "  df_comparison.columns = ['ID'] # Naming the column in df_p_test\n",
        "  df_comparison[\"y_test\"]=y_test[\"y_test\"] # Adding the dataframe y_test\n",
        "  df_comparison[\"y_pred\"]=y_pred[\"y_pred\"] # Adding the dataframe y_pred.\n",
        "  df_comparison['comparison'] = np.where(df_comparison['y_test'] == df_comparison['y_pred'], 1, 0) # Adding the comparison coloumn, where y_pred==y_test, it's true.\n",
        "\n",
        "  return df_comparison, model\n",
        "\n",
        "def patient_score_beta(df_comparison):\n",
        "  \"\"\"\n",
        "  Computes patient score based on the patients' ID as list. An extra list with the patient score and the ID as tuple is given to analyse individual response.\n",
        "  :param df_comparison: Dataframe with 3 columns: ID, prediction and real label\n",
        "  :return: two list with the pscores and pscores with ID\n",
        "  \"\"\"\n",
        "  p_score_list = [] #List with the patient scores\n",
        "  p_score_ID_list = [] #List with the patient score and IDs\n",
        "\n",
        "  for d in df_comparison.groupby('ID'): # Groupby the patients based on their Id's\n",
        "    true_counts=d[1].comparison.sum() # Creating Nrec value. \n",
        "    num_img=d[1].ID.value_counts(dropna=False)[0] #Creating Np value.\n",
        "    p_score=true_counts/num_img # For each index, calculate p score, Nrec/Np\n",
        "    p_score_list.append(p_score) #Append at the end of the patient score list\n",
        "    p_score_ID_list.append([p_score,d[0]]) #Append at the end of the ID list\n",
        "  \n",
        "  return p_score_list, p_score_ID_list\n",
        "\n",
        "def performance_metrics(df_comparison):\n",
        "  \"\"\"\n",
        "  Here the performance metrics are computed. Currently patient score and recognition rate are the basis.\n",
        "  Image-wise accuracy has also been added.\n",
        "  :param df_comparison: Dataframe with 3 columns: ID, prediction and real label\n",
        "  :return: performance metrics\n",
        "  \"\"\"\n",
        "  p_score_list, p_score_ID_list = patient_score_beta(df_comparison) #Get patient scores\n",
        "  rec_rate = np.mean(p_score_list) #Get recognition rate\n",
        "  acc = df_comparison.comparison.mean()\n",
        "\n",
        "  return p_score_list, p_score_ID_list, rec_rate, acc\n",
        "\n",
        "def get_problematic_patients(p_score_ID_list, min_score=0.5):\n",
        "  \"\"\"\n",
        "  Given a pscore with patients ID, the ID and performance of problematic (pscore<min_score) patients os given.\n",
        "  :param p_score_ID_list: list of pscores and IDs\n",
        "  :return: list of problematic patients\n",
        "  \"\"\"\n",
        "  problem_list = []\n",
        "  for p in p_score_ID_list:\n",
        "    if p[0]<0.5:\n",
        "      problem_list.append(p)\n",
        "  return problem_list\n",
        "\n",
        "def model2performance_metrics(extractor='dense', read_files_type=read_files_pickle, pipe=SVC(), max_corr=1, fold=0, mag=0):\n",
        "  \"\"\"\n",
        "  Input ML model settings, as well as folder to extract\n",
        "  :param extractor: name of extractor of features used\n",
        "  :param read_files_type: type of file to be read (csv, pickle)\n",
        "  :param pipe: Pipeline of the classification method.\n",
        "  :param max_corr: Maximum correlation allowed between features\n",
        "  :param fold, mag: known\n",
        "  :return: metrics of the model + model. Additionally, problematic patients are displayed.\n",
        "  \"\"\"\n",
        "  X_train, X_test, y_train, y_test, _, ID_test = read_files_type(extractor=extractor, fold=fold, mag=mag) #Split data\n",
        "  df_comparison, model = machine_learning_pipeline(X_train, X_test, y_train, y_test, ID_test, pipe=pipe, max_corr=max_corr) #Main pipeline. Obtain comparison of labels\n",
        "  p_score_list, p_score_ID_list, rec_rate, acc = performance_metrics(df_comparison) #Get performance metrics\n",
        "  p_problem = get_problematic_patients(p_score_ID_list)\n",
        "\n",
        "  print(f'-For fold {fold+1} and magnification {mag_dict[mag]}:\\n') #Print metrics\n",
        "  print(f'Recognition rate: {rec_rate}')\n",
        "  print(f'Image-wise accuracy: {acc}')\n",
        "  print(f'Problematic patients: {p_problem}\\n') #Show problematic patients' ID and score\n",
        "\n",
        "  return p_score_list, p_score_ID_list, rec_rate, acc, model\n",
        "\n",
        "def result_all_folders(extractor, read_files_type, pipe, max_corr=1, mag=0):\n",
        "  \"\"\"\n",
        "  All 5 folds are run for the same magnification. The extractor name, pipe definition as well as the max_correaltion have to be given\n",
        "  :param extractor: name of extractor of features used\n",
        "  :param read_files_type: type of file to be read (csv, pickle)\n",
        "  :param pipe: Pipeline of the classification method.\n",
        "  :param max_corr: Maximum correlation allowed between features\n",
        "  :param mag: known\n",
        "  :return: NONE\n",
        "  \"\"\"\n",
        "  all_rec_rates = np.zeros(5) #To save recognition rates\n",
        "  for fold in range(5):\n",
        "    p_score_list, p_score_ID_list, rec_rate, acc, model = model2performance_metrics(extractor, read_files_type, pipe=pipe, max_corr=max_corr, fold=fold, mag=mag) #Fold-wise learning method\n",
        "    all_rec_rates[fold] = rec_rate #save recognition rate\n",
        "    if hasattr(model,'best_estimator_'):\n",
        "      print(f'Grid best estimator: {model.best_estimator_}\\n') #Print the best estimator hyperparameters of the grid search\n",
        "      print(f'Best estimator number of Principal COmponents: {model.best_estimator_.named_steps[\"reductor\"].explained_variance_ratio_.shape[0]}\\n')\n",
        "      print('############\\n')\n",
        "\n",
        "  print(f'---- Mean recognition rate for magnification {mag_dict[mag]}x: {all_rec_rates.mean()}') #Mean recognition rate (final metric)\n",
        "  return model"
      ],
      "metadata": {
        "id": "2106YMYP4pwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiclass function"
      ],
      "metadata": {
        "id": "Va9YcbgI5R1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_unifier(extractor, read_files_type,fold, mag):\n",
        "  \"\"\"\n",
        "  Given a feature extractor name and specifi fold and magnification, returns the train and test dataframes of the whole data information.\n",
        "  This information is ID, X, and the two endpoints (binary and multiclass)\n",
        "  :param extractor: name of feature extracted\n",
        "  :param fold and mag: fold and mag used\n",
        "  :return: two dataframes with all this information put together.\n",
        "  \"\"\"\n",
        "\n",
        "  X_train, X_test, y_train, y_test, ID_train, ID_test = read_files_type(extractor=extractor, fold=fold, mag=mag) #Read files\n",
        "  #Ensure extracted files are numpy and give proper shape\n",
        "  X_train = np.array(X_train)\n",
        "  X_test = np.array(X_test)\n",
        "  y_train = np.array(y_train).reshape(-1,1)\n",
        "  y_test = np.array(y_test).reshape(-1,1)\n",
        "  ID_train = np.array(ID_train)\n",
        "  ID_test = np.array(ID_test)\n",
        "  #Continue\n",
        "  multi_train, multi_test = train_test_from_pickle(category='endpoints_multi', fold=fold, mag=mag) #Read multi-endpoints\n",
        "  train_df = pd.DataFrame(np.concatenate((ID_train,X_train,y_train,multi_train),axis=1)) #Unify matrices in DataFrame\n",
        "  test_df = pd.DataFrame(np.concatenate((ID_test,X_test,y_test,multi_test),axis=1)) #Unify matrices in DataFrame\n",
        "\n",
        "\n",
        "  mapping = {train_df.columns[0]: 'ID', train_df.columns[-2]: 'Binary', train_df.columns[-1]: 'Multi'} #New columns names\n",
        "  train_df = train_df.rename(columns=mapping) #Rename\n",
        "  test_df = test_df.rename(columns=mapping) #Rename\n",
        "\n",
        "  return train_df, test_df\n",
        "\n",
        "def binary_dataloader(extractor, read_files_type, fold, mag, bm):\n",
        "  \"\"\"\n",
        "  Give the train and test split for a specific feature extractor, fold, maginificaiton and type of binary problem (benignant or malignant)\n",
        "  :return: Data split\n",
        "  \"\"\"\n",
        "  train, test = dataframe_unifier(extractor=extractor, read_files_type=read_files_type, fold=fold, mag=mag) #Get train and test dataframes\n",
        "  #Train columns\n",
        "  binary_train = train[train['Binary']==bm]\n",
        "  X_train = binary_train.iloc[:,1:-2].astype('float').reset_index(drop=True)\n",
        "  y_train = binary_train.iloc[:,-1].astype('int32').reset_index(drop=True)\n",
        "  #if bm==1 : y_train = y_train - 4 #Move endpoints to zero if malignant\n",
        "  ID_train = binary_train.iloc[:,0].reset_index(drop=True)\n",
        "  #Test columns\n",
        "  binary_test = test[test['Binary']==bm]\n",
        "  X_test = binary_test.iloc[:,1:-2].astype('float').reset_index(drop=True)\n",
        "  y_test = binary_test.iloc[:,-1].astype('int32').reset_index(drop=True)\n",
        "  #if bm==1 : y_test = y_test - 4\n",
        "  ID_test = binary_test.iloc[:,0].reset_index(drop=True)\n",
        "\n",
        "  return X_train, X_test, y_train, y_test, ID_train, ID_test\n",
        "\n",
        "#Define identity transformation\n",
        "class IdentityTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def fit(self, input_array, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, input_array, y=None):\n",
        "        return input_array*1\n",
        "\n",
        "def classifier_and_grid_2(X, y, ID, method, grid='normal',verbose=1, reductor_components=[1],scoring='accuracy', sampler = SMOTE()):\n",
        "  \"\"\"\n",
        "  Definition of the classifier grid method for the hyperparameters search.\n",
        "  :param method: learning method name (SVM, random forest, etc.)\n",
        "  :param grid: type of grid search given (random or normal)\n",
        "  \"\"\"\n",
        "  #CV\n",
        "  gkf = list(StratifiedGroupKFold(n_splits=4).split(X,y,groups=ID))\n",
        "  #Decision function for SVM\n",
        "  decision_function = ['ovr']\n",
        "  #Scorer\n",
        "  scoring = make_scorer(matthews_corrcoef) if scoring=='MCC' else None\n",
        "  #Sampler for unbalanced data\n",
        "  \n",
        "  #Random iterations\n",
        "  n_iter = 30\n",
        "\n",
        "  #KNN\n",
        "  if(method=='KNN'):\n",
        "    param_grid = {'classifier__n_neighbors': list(range(1,30))}\n",
        "    pipe = Pipelineim([('scaler', StandardScaler()),('sampler',sampler),('classifier',KNeighborsClassifier())])\n",
        "    grid = GridSearchCV(pipe, param_grid, scoring=scoring, cv = gkf,verbose = verbose)\n",
        "    return grid, gkf\n",
        "  #KNN with PCA\n",
        "  elif(method=='KNN_PCA'):\n",
        "    param_grid = {'reductor__n_components': reductor_components,'classifier__n_neighbors': list(range(1,30))}\n",
        "    pipe = Pipelineim([('scaler', StandardScaler()),('reductor',PCA()), ('sampler',sampler), ('classifier',KNeighborsClassifier())])\n",
        "    grid = GridSearchCV(pipe, param_grid, scoring=scoring, cv = gkf,verbose = verbose)\n",
        "    return grid, gkf\n",
        "\n",
        "  #SVM with PCA\n",
        "  if(method=='SVM_PCA'):\n",
        "    if(grid=='normal'):\n",
        "      param_grid = {'reductor__n_components': reductor_components,'classifier__C': [0.1, 1, 10, 100],\n",
        "              'classifier__gamma': [100, 10, 1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'classifier__kernel': ['rbf'], 'classifier__class_weight':['balanced']}\n",
        "      pipe = Pipelineim([('scaler', StandardScaler()),('reductor',PCA()), ('sampler',sampler), ('classifier',SVC())])\n",
        "      grid = GridSearchCV(pipe, param_grid, scoring=scoring, cv = gkf ,verbose = verbose)\n",
        "      return grid, gkf\n",
        "    elif(grid=='random'):\n",
        "      parameters = {'reductor__n_components': reductor_components,'classifier__C': scipy.stats.expon(scale=10), 'classifier__gamma': scipy.stats.expon(scale=.001), #Parameters for grid search\n",
        "      'classifier__kernel': ['rbf'], 'classifier__class_weight':['balanced']}\n",
        "      pipe = Pipelineim([('scaler', StandardScaler()),('reductor',PCA()), ('sampler',sampler), ('classifier',SVC())]) #Definition of pipeline\n",
        "      grid = RandomizedSearchCV(pipe, parameters,n_iter=n_iter, scoring=scoring, cv = gkf , verbose=verbose,return_train_score=False) #Random search\n",
        "      return grid, gkf\n",
        "  #SVM no PCA\n",
        "  elif(method=='SVM'):\n",
        "    if(grid=='normal'):\n",
        "      param_grid = {'classifier__C': [0.1, 1, 10, 100],\n",
        "              'classifier__gamma': [100, 10, 1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'classifier__kernel': ['rbf'],'classifier__class_weight':['balanced']}\n",
        "      pipe = Pipelineim([('scaler', StandardScaler()), ('sampler',sampler), ('classifier',SVC())])\n",
        "      grid = GridSearchCV(pipe, param_grid, scoring=scoring, cv = gkf ,verbose = verbose)\n",
        "      return grid, gkf\n",
        "    elif(grid=='random'):\n",
        "      parameters = {'classifier__C': scipy.stats.expon(scale=10), 'classifier__gamma': scipy.stats.expon(scale=.001), #Parameters for grid search\n",
        "      'classifier__kernel': ['rbf'],'classifier__class_weight':['balanced']}\n",
        "      pipe = Pipelineim([('scaler', StandardScaler()), ('sampler',sampler),('classifier',SVC())]) #Definition of pipeline\n",
        "      grid = RandomizedSearchCV(pipe, parameters,n_iter=n_iter, scoring=scoring, cv = gkf , verbose=verbose,return_train_score=False) #Random search\n",
        "      return grid, gkf\n",
        "\n",
        "\n",
        "  #Random forest\n",
        "  elif(method=='RF'):\n",
        "    param_grid = {'classifier__n_estimators': [100, 200, 400, 600, 800],}\n",
        "    pipe = Pipelineim([('scaler', StandardScaler()), ('sampler',sampler),('classifier',RandomForestClassifier())])\n",
        "    grid = GridSearchCV(pipe, param_grid, scoring=scoring, cv = gkf, verbose = verbose)\n",
        "    return grid, gkf\n",
        "  #Random forest with PCA\n",
        "  elif(method=='RF_PCA'):\n",
        "    param_grid = {'reductor__n_components': reductor_components,'classifier__n_estimators': [100, 200, 400, 600, 800],}\n",
        "    pipe = Pipelineim([('scaler', StandardScaler()),('reductor',PCA()), ('sampler',sampler),('classifier',RandomForestClassifier())])\n",
        "    grid = GridSearchCV(pipe, param_grid, scoring=scoring, cv = gkf, verbose = verbose)\n",
        "    return grid, gkf\n",
        "\n",
        "def model2performance_metrics_multi(extractor='dense', method = 'SVM', grid = 'random', verbose = 1, reductor_components = [1], scoring = 'MCC', sampler = SMOTE(), max_corr=0.99, fold=0, mag=0, bm=0, multi_label=0):\n",
        "  \"\"\"\n",
        "  Input ML model settings, as well as folder to extract\n",
        "  :param extractor: name of extractor of features used\n",
        "  :param read_files_type: type of file to be read (csv, pickle)\n",
        "  :param pipe: Pipeline of the classification method.\n",
        "  :param max_corr: Maximum correlation allowed between features\n",
        "  :param fold, mag: known\n",
        "  :return: metrics of the model + model. Additionally, problematic patients are displayed.\n",
        "  \"\"\"\n",
        "\n",
        "  #Data split\n",
        "  X_train, X_test, y_train, y_test, ID_train, ID_test = binary_dataloader(extractor=extractor, read_files_type = read_files_pickle, fold=fold, mag=mag, bm=bm) #Split data in train and test\n",
        "  #Multiclass labels one vs all\n",
        "  y_train_one = y_train==multi_label\n",
        "  y_test_one = y_test==multi_label\n",
        "\n",
        "  #Pipeline definition and \n",
        "  pipe, gfk = classifier_and_grid_2(X_train, y_train_one, ID_train, method=method, grid=grid, verbose=verbose, reductor_components=reductor_components,scoring=scoring, sampler = sampler) #Pipe is defined after data loading because it is needed for the split\n",
        "  df_comparison, model = machine_learning_pipeline(X_train, X_test, y_train_one, y_test_one, ID_test, pipe, max_corr=1)\n",
        "\n",
        "  p_score_list, p_score_ID_list, rec_rate, acc = performance_metrics(df_comparison) #Get performance metrics\n",
        "  p_problem = get_problematic_patients(p_score_ID_list)\n",
        "\n",
        "  print(f'-For fold {fold+1} and magnification {mag_dict[mag]}:\\n') #Print metrics\n",
        "  print(f'Recognition rate: {rec_rate}')\n",
        "  print(f'Image-wise accuracy: {acc}')\n",
        "  print(f'Problematic patients: {p_problem}\\n') #Show problematic patients' ID and score\n",
        "\n",
        "  return p_score_list, p_score_ID_list, rec_rate, acc, model\n",
        "\n",
        "def result_all_folders_strat(extractor = 'dense', method = 'SVM', grid = 'normal', verbose = 1, reductor_components = [1], scoring = 'accuracy', sampler = IdentityTransformer(), max_corr=0.99, mag=0, bm=0, multi_label=0):\n",
        "  \"\"\"\n",
        "  All 5 folds are run for the same magnification. The extractor name, pipe definition as well as the max_correaltion have to be given\n",
        "  :param extractor: name of extractor of features used\n",
        "  :param read_files_type: type of file to be read (csv, pickle)\n",
        "  :param pipe: Pipeline of the classification method.\n",
        "  :param max_corr: Maximum correlation allowed between features\n",
        "  :param mag: known\n",
        "  :return: NONE\n",
        "  \"\"\"\n",
        "  all_rec_rates = np.zeros(5) #To save recognition rates\n",
        "  for fold in range(5):\n",
        "    p_score_list, p_score_ID_list, rec_rate, acc, model = model2performance_metrics_multi(extractor=extractor, method = method, grid = grid, verbose = verbose, reductor_components = reductor_components, scoring = scoring, sampler = sampler, max_corr=0.99, fold=fold, mag=mag, bm=bm, multi_label=multi_label) #Fold-wise learning method\n",
        "    all_rec_rates[fold] = rec_rate #save recognition rate\n",
        "    if hasattr(model,'best_estimator_'):\n",
        "      print(f'Grid best estimator: {model.best_estimator_}\\n') #Print the best estimator hyperparameters of the grid search\n",
        "      #print(f'Best estimator number of Principal Components: {model.best_estimator_.named_steps[\"reductor\"].explained_variance_ratio_.shape[0]}\\n')\n",
        "      print('############\\n')\n",
        "  rec_rate_mean = all_rec_rates.mean()\n",
        "  rec_rate_std = all_rec_rates.std()\n",
        "  print(f'---- Mean recognition rate for magnification {mag_dict[mag]}x: {rec_rate_mean}') #Mean recognition rate (final metric)\n",
        "  return rec_rate_mean, rec_rate_std"
      ],
      "metadata": {
        "id": "1tt_4OdH5QHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "B21Tpx4r5fFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train binary models for each main binary class (benign-malignant) and the histopatological subtype"
      ],
      "metadata": {
        "id": "-bskZ4FY53OS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm = 1 #Which binary class\n",
        "multi_label = 5 #Which subtype #For benign 0-3, for malignant 4-7\n",
        "\n",
        "super_multi = np.zeros((12,9),dtype='object') #Matrix containing all results\n",
        "i = 0 #Matrix row counter\n",
        "for extractor in ['dense']:\n",
        "  for method in ['RF','RF_PCA']:\n",
        "    for sampler in [SMOTE(),IdentityTransformer()]:\n",
        "      \n",
        "      #Magnification level model comparison\n",
        "      rec_rate_all = np.zeros((4,2)) #Row is magnification, column is mean and standard deviation\n",
        "      for mag in [0,1,2,3]:\n",
        "\n",
        "        grid = 'random'\n",
        "        verbose = 1\n",
        "        reductor_components = [0.95] #For PCA only\n",
        "        scoring = 'MCC'\n",
        "        max_corr = 0.99\n",
        "\n",
        "        #Predict for magnification\n",
        "        rec_rate_all[mag,:] = result_all_folders_strat(extractor = extractor, method = method, grid = grid, verbose = verbose, reductor_components = reductor_components, scoring = scoring, sampler =sampler, max_corr=max_corr, mag = mag, bm=bm, multi_label=multi_label)\n",
        "      super_multi[i,1:] = rec_rate_all.ravel()\n",
        "      super_multi[i,0] = extractor+'_'+method+'_'+str(sampler)\n",
        "      i = i+1\n",
        "with open(f'/content/drive/MyDrive/Ars_machinae_autodiscentis/Inceptum/Machine_Learning/Results/super_multi_{bm}_{multi_label}.p','wb') as handle:\n",
        "  pickle.dump(super_multi, handle, pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "WmHAP4G5Q_eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting best models"
      ],
      "metadata": {
        "id": "MXtv7ef-z_It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Matrix where we save the best models for multiclasses\n",
        "best_pred_multi = np.zeros((4,2,2,4),dtype='object')"
      ],
      "metadata": {
        "id": "nvDA0XtJ1C8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fold = 0\n",
        "#Unchangable settings\n",
        "extractor = 'dense'\n",
        "grid = 'random'\n",
        "reductor_components = [0.95, 0.99]\n",
        "scoring = 'MCC'\n",
        "verbose = 3\n",
        "\n",
        "#Settings to change\n",
        "mag = 3\n",
        "bm = 1\n",
        "multi_label = 0\n",
        "method = 'RF'\n",
        "sampler = IdentityTransformer()\n",
        "max_corr = 0.99\n",
        "\n",
        "\n",
        "p_score_list, p_score_ID_list, rec_rate, acc, model = model2performance_metrics_multi(extractor=extractor, method = method, grid = grid, verbose = verbose, reductor_components = reductor_components, scoring = scoring, sampler = sampler, max_corr=0.99, fold=fold, mag=mag, bm=bm, multi_label=multi_label+4) #Fold-wise learning method\n",
        "best_pred_multi[mag, bm, :,multi_label] = [model,rec_rate]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YXNG8Ys0B7F",
        "outputId": "41f57bd9-adf5-45e1-b638-6e61607c68d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input features: 1024\n",
            "\n",
            "No-correlated features: 1024\n",
            "\n",
            "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
            "[CV 1/4] END ......classifier__n_estimators=100;, score=0.517 total time=   1.3s\n",
            "[CV 2/4] END ......classifier__n_estimators=100;, score=0.469 total time=   1.3s\n",
            "[CV 3/4] END .....classifier__n_estimators=100;, score=-0.160 total time=   1.3s\n",
            "[CV 4/4] END ......classifier__n_estimators=100;, score=0.268 total time=   1.3s\n",
            "[CV 1/4] END ......classifier__n_estimators=200;, score=0.569 total time=   2.6s\n",
            "[CV 2/4] END ......classifier__n_estimators=200;, score=0.440 total time=   2.5s\n",
            "[CV 3/4] END .....classifier__n_estimators=200;, score=-0.041 total time=   2.5s\n",
            "[CV 4/4] END ......classifier__n_estimators=200;, score=0.188 total time=   2.5s\n",
            "[CV 1/4] END ......classifier__n_estimators=400;, score=0.479 total time=   5.2s\n",
            "[CV 2/4] END ......classifier__n_estimators=400;, score=0.509 total time=   5.0s\n",
            "[CV 3/4] END .....classifier__n_estimators=400;, score=-0.107 total time=   5.1s\n",
            "[CV 4/4] END ......classifier__n_estimators=400;, score=0.216 total time=   5.0s\n",
            "[CV 1/4] END ......classifier__n_estimators=600;, score=0.452 total time=   7.8s\n",
            "[CV 2/4] END ......classifier__n_estimators=600;, score=0.474 total time=   7.7s\n",
            "[CV 3/4] END .....classifier__n_estimators=600;, score=-0.108 total time=   7.5s\n",
            "[CV 4/4] END ......classifier__n_estimators=600;, score=0.175 total time=   7.4s\n",
            "[CV 1/4] END ......classifier__n_estimators=800;, score=0.478 total time=  10.3s\n",
            "[CV 2/4] END ......classifier__n_estimators=800;, score=0.474 total time=  10.0s\n",
            "[CV 3/4] END .....classifier__n_estimators=800;, score=-0.084 total time=  10.1s\n",
            "[CV 4/4] END ......classifier__n_estimators=800;, score=0.196 total time=  10.0s\n",
            "-For fold 1 and magnification 400:\n",
            "\n",
            "Recognition rate: 0.7534069579734889\n",
            "Image-wise accuracy: 0.6794258373205742\n",
            "Problematic patients: [[0.0, '13412'], [0.0, '13413'], [0.3902439024390244, '15570'], [0.05714285714285714, '15704']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Ars_machinae_autodiscentis/Inceptum/Machine_Learning/Results/Best_models/Best_pred_multi_2.p','wb') as handle:\n",
        "  pickle.dump(best_pred_multi, handle, pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "rZA87JS41Vwj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}